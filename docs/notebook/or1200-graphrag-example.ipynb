{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArangoGraphRAG: Workflow Tutorial\n",
    "\n",
    "This notebook provides a complete guideâ€”using clear Python examplesâ€”covering each major step in the GraphRAG pipeline with Arango, including GenAI API usage, service orchestration, document import & conversion, and RAG querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GenAI API\n",
    "\n",
    "### Setup and Authentication\n",
    "\n",
    "First, install the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install fitz\n",
    "! pip install PyMuPDF\n",
    "! pip install PyPDF2\n",
    "! pip install markdownify\n",
    "! pip install docling==2.26.0\n",
    "! pip install aiohttp\n",
    "! pip install pymupdf4llm pymupdf_layout ocrmypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress all warnings to keep output clean\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "import requests\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "\n",
    "# Suppress DeprecationWarning and other warnings\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter('ignore')\n",
    "\n",
    "print(\"âœ“ Warning suppression enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary modules and set up environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import json\n",
    "import base64\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import pymupdf4llm\n",
    "from typing import Dict, Optional, List, Tuple\n",
    "from dotenv import load_dotenv\n",
    "from docling.document_converter import DocumentConverter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up environment variable helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_updated_env_var(env_var_name):\n",
    "    load_dotenv(dotenv_path=\"./env_cadence_poc\", override=True)\n",
    "    return os.environ.get(env_var_name, None)\n",
    "\n",
    "# Initialize JWT token variable\n",
    "jwt_token = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate with the GenAI API and retrieve a JWT token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate():\n",
    "    global jwt_token\n",
    "    auth_url = f\"{get_updated_env_var('SERVER_URL')}/_open/auth\"\n",
    "    payload = {\n",
    "        \"username\": get_updated_env_var(\"USERNAME\"),\n",
    "        \"password\": get_updated_env_var(\"PASSWORD\")\n",
    "    }\n",
    "    try:\n",
    "        print(\"Authenticating with the server...\")\n",
    "        response = requests.post(auth_url, json=payload, verify=False)\n",
    "        response.raise_for_status()\n",
    "        jwt_token = response.json().get(\"jwt\")\n",
    "        if not jwt_token:\n",
    "            raise ValueError(\"Authentication response does not contain a token.\")\n",
    "        print(\"âœ“ Authentication successful. JWT token retrieved.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error during authentication: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for authenticated API requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_request(suffix: str, payload: Dict, method: str = \"POST\") -> Optional[Dict]:\n",
    "    \"\"\"Sends an HTTP request with JWT authorization and returns the response.\"\"\"\n",
    "    authenticate()\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {jwt_token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    url = f\"{get_updated_env_var('SERVER_URL')}{suffix}\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"INFO: Sending {method} request to {suffix}\")\n",
    "        response = requests.request(method, url, json=payload, headers=headers, verify=False)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"ERROR: Error during request to {url}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def send_streaming_request_async(\n",
    "    suffix: str, payload: Dict, method: str = \"POST\"\n",
    ") -> Tuple[List[Dict], str]:\n",
    "    \"\"\"\n",
    "    Sends an async HTTP request expecting a streaming response and returns parsed JSON objects.\n",
    "    \n",
    "    Args:\n",
    "        suffix: The URL suffix to send the request to\n",
    "        payload: The request payload\n",
    "        method: HTTP method (default: POST)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (parsed_json_objects, raw_response_text)\n",
    "    \"\"\"\n",
    "    authenticate()\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"text/event-stream\",\n",
    "        \"Authorization\": f\"Bearer {jwt_token}\",\n",
    "    }\n",
    "    url = f\"{get_updated_env_var('SERVER_URL')}{suffix}\"\n",
    "    try:\n",
    "        print(f\"Sending {method} async streaming request to {url}\")\n",
    "        \n",
    "        timeout = aiohttp.ClientTimeout(total=7200)\n",
    "        async with aiohttp.ClientSession(timeout=timeout) as session:\n",
    "            async with session.request(method, url, json=payload, headers=headers) as response:\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Read the streaming response\n",
    "                raw_response = \"\"\n",
    "                json_objects = []\n",
    "                \n",
    "                async for line in response.content:\n",
    "                    if line:\n",
    "                        line_str = line.decode(\"utf-8\").strip()\n",
    "                        raw_response += line_str + \"\\n\"\n",
    "                        #print(line_str)\n",
    "                        \n",
    "                        # Skip empty lines and SSE comments\n",
    "                        if not line_str or line_str.startswith(\":\"):\n",
    "                            continue\n",
    "                        \n",
    "                        # Remove SSE data prefix if present\n",
    "                        if line_str.startswith(\"data: \"):\n",
    "                            line_str = line_str[6:]\n",
    "                        \n",
    "                        # Parse JSON object\n",
    "                        try:\n",
    "                            json_obj = json.loads(line_str)\n",
    "                            json_objects.append(json_obj)\n",
    "                            print(f\"ðŸ“¨ Received response: {json_obj}\")\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"Failed to parse JSON: {line_str} - Error: {e}\")\n",
    "                            continue\n",
    "        return json_objects, raw_response\n",
    "        \n",
    "    except aiohttp.ClientError as e:\n",
    "        print(f\"âŒ Error during async streaming request to {url}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Starting a Service\n",
    "\n",
    "Define the start_service function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_service(service_name: str, startup_parameters: Dict) -> Optional[Dict]:\n",
    "    \"\"\"Start a service using the gen-ai API.\"\"\"\n",
    "    body = {\n",
    "        \"service_name\": service_name,\n",
    "        \"env\": startup_parameters\n",
    "    }\n",
    "    return send_request(\"/gen-ai/v1/service\", body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a GenAI project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_PROJECT_NAME = get_updated_env_var(\"DB_PROJECT_NAME\")\n",
    "DB_NAME = get_updated_env_var(\"DB_NAME\")\n",
    "\n",
    "# Create GenAI Project\n",
    "project_payload = {\n",
    "    \"project_name\": DB_PROJECT_NAME,\n",
    "    \"project_db_name\": DB_NAME,\n",
    "    \"project_type\": \"Graph\",  # TODO: Enter your project type\n",
    "    \"project_description\": \"GraphRAG Project\"  # TODO: Enter your project description\n",
    "}\n",
    "\n",
    "print(f\"Creating project '{DB_PROJECT_NAME}' in database '{DB_NAME}'...\")\n",
    "try:\n",
    "    project_response = send_request(\"/gen-ai/v1/project\", project_payload, \"POST\")\n",
    "    print(f\"Project '{DB_PROJECT_NAME}' created successfully!\")\n",
    "    print(json.dumps(project_response, indent=2))\n",
    "except Exception as e:\n",
    "    error_str = str(e).lower()\n",
    "    print(f\"Project '{DB_PROJECT_NAME}' already exists. Continuing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the Importer service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the GraphRAG Importer service\n",
    "importer_params = {\n",
    "    \"db_name\": get_updated_env_var(\"DB_NAME\"),\n",
    "    \"username\": get_updated_env_var(\"USERNAME\"),\n",
    "    \"password\": get_updated_env_var(\"PASSWORD\"),\n",
    "    \"chat_model\": \"gpt-4.1\",\n",
    "    \"chat_api_provider\": \"openai\",\n",
    "    \"embedding_api_provider\": \"openai\",\n",
    "    \"chat_api_key\": get_updated_env_var(\"OPENAI_API_KEY\"),\n",
    "    \"embedding_api_key\": get_updated_env_var(\"OPENAI_API_KEY\"),\n",
    "    \"genai_project_name\": get_updated_env_var(\"DB_PROJECT_NAME\"),\n",
    "    #\"profiles\": \"memory-2gi-cpu-1\"\n",
    "}\n",
    "\n",
    "importer_response = start_service(\"arangodb-graphrag-importer\", importer_params)\n",
    "print(json.dumps(importer_response, indent=2))\n",
    "\n",
    "# Extract service ID\n",
    "importer_service_id = importer_response[\"serviceInfo\"][\"serviceId\"].split(\"-\")[-1]\n",
    "print(f\"\\nImporter Service ID: {importer_service_id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. List of Services\n",
    "\n",
    "Define list_services function and retrieve running services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_services() -> Optional[Dict]:\n",
    "    \"\"\"List all services using the gen-ai API.\"\"\"\n",
    "    return send_request(\"/gen-ai/v1/list_services\", {}, \"POST\")\n",
    "\n",
    "services_response = list_services()\n",
    "print(\"All Services:\")\n",
    "print(json.dumps(services_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating an Importer\n",
    "\n",
    "The Importer service has already been started in section 2. Below is the update function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Updating an Importer\n",
    "\n",
    "Define the upgrade_service function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_service(full_service_id: str, startup_parameters: Dict) -> Optional[Dict]:\n",
    "    \"\"\"Update a service by service_id with new configuration, environment variables, or labels.\n",
    "    \n",
    "    Args:\n",
    "        service_id: The unique identifier of the service (path parameter)\n",
    "        startup_parameters: Dictionary containing new configuration, env vars, or labels\n",
    "    \n",
    "    Returns:\n",
    "        Response from the upgrade service API\n",
    "    \"\"\"\n",
    "    body = {\"env\": startup_parameters}\n",
    "    return send_request(f\"/gen-ai/v1/service/{full_service_id}\", body, \"PUT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Update the Importer service with new parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare upgraded parameters\n",
    "updated_params = importer_params.copy()\n",
    "updated_params[\"chat_model\"] = \"gpt-4o\"\n",
    "updated_params[\"profiles\"] = \"memory-8gi-cpu-2\"\n",
    "\n",
    "importer_service_id_full = f\"arangodb-graphrag-importer-{importer_service_id}\"\n",
    "print(f\"Updating service: {importer_service_id_full}\")\n",
    "response = update_service(importer_service_id_full, updated_params)\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Viewing a Pod in k9s\n",
    "\n",
    "To view pod details in `k9s`, launch the tool and use the following keyboard shortcuts:\n",
    "\n",
    "- **Arrow keys**: Navigate through pods\n",
    "- **ENTER**: View pod details\n",
    "- **d**: Describe the pod (shows full YAML, status, and events)\n",
    "- **l**: View logs; press `0`, `1`, `2`... to filter by time range\n",
    "- **ESC**: Return to previous view\n",
    "- **SHIFT+F**: Port forward a pod\n",
    "- **e**: Edit pod configuration\n",
    "- **CTRL+D**: Delete the pod\n",
    "- **?**: Show help menu with all available commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing files\n",
    "\n",
    "Convert input documents (PDF/TXT) to Markdown using Docling or pymupdf4llm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_markdown_docling(pdf_file):\n",
    "    \"\"\"Convert PDF to Markdown using Docling\"\"\"\n",
    "    converter = DocumentConverter()\n",
    "    result = converter.convert(pdf_file)\n",
    "    output_md_file = pdf_file.replace(\".pdf\", \"_docling.md\")\n",
    "    with open(output_md_file, \"w\", encoding=\"utf-8\") as md_file:\n",
    "        md_file.write(result.document.export_to_markdown())\n",
    "    print(f\"Converted {pdf_file} to {output_md_file}\")\n",
    "    return output_md_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_markdown_pymupdf4llm(pdf_file, page_chunks=False):\n",
    "    chunks = pymupdf4llm.to_markdown(pdf_file, page_chunks=page_chunks)\n",
    "    output_md_file = pdf_file.replace(\".pdf\", \"_pymupdf4llm.md\")\n",
    "    \n",
    "    # If page_chunks=True, it returns a list\n",
    "    if isinstance(chunks, list):\n",
    "        with open(output_md_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for i, chunk in enumerate(chunks, 1):\n",
    "                f.write(f\"## Page {i}\\n\\n{chunk}\\n\\n\")\n",
    "    else:\n",
    "        # If page_chunks=False, it returns a string\n",
    "        with open(output_md_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(chunks)\n",
    "    \n",
    "    return output_md_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_file_content(filepath: str) -> Optional[str]:\n",
    "    \"\"\"Encodes the file content to Base64.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as file:\n",
    "            encoded_content = base64.b64encode(file.read()).decode(\"utf-8\")\n",
    "            print(f\"âœ“ Successfully encoded file: {filepath}\")\n",
    "            return encoded_content\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âœ— File not found: {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error reading file {filepath}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder(\n",
    "    input_folder: str,\n",
    "    json_output_file: str,\n",
    "    output_folder: str = \"./markdown_output\",\n",
    "    use_docling_for_pdf: bool = False,\n",
    "    skip_conversion: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Processes a folder full of PDF and/or HTML files, converts them to Markdown,\n",
    "    and creates a JSON file with information about each converted file.\n",
    "    \n",
    "    Args:\n",
    "        input_folder: Path to the folder containing PDF and/or HTML files (or markdown files if skip_conversion=True)\n",
    "        json_output_file: Path to the output JSON file\n",
    "        output_folder: Path to the folder where Markdown files will be saved (default: \"./markdown_output\")\n",
    "                       This folder is deleted and recreated each time when skip_conversion=False\n",
    "        use_docling_for_pdf: If True, uses docling for PDF. If False, uses pymupdf4llm (default)\n",
    "        skip_conversion: If True, reads markdown files directly from input_folder (output_folder not needed)\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict]: List of files with \"name\", \"content\", and \"citable_url\" keys\n",
    "    \"\"\"\n",
    "    # List to store information about each file\n",
    "    files_info = []\n",
    "    \n",
    "    if skip_conversion:\n",
    "        # Process existing markdown files from input folder\n",
    "        input_path = pathlib.Path(input_folder)\n",
    "        md_files = list(input_path.glob(\"*.md\"))\n",
    "        \n",
    "        if not md_files:\n",
    "            print(f\"âš  No markdown files found in {input_folder}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"ðŸ“ Processing {len(md_files)} existing markdown file(s) from {input_folder}...\")\n",
    "        \n",
    "        # Process each markdown file\n",
    "        for md_file_path in md_files:\n",
    "            md_file_name = md_file_path.name\n",
    "            # Try to find original file name (remove .md extension and try to match with PDF/HTML)\n",
    "            base_name = md_file_name.rsplit('.', 1)[0]\n",
    "            \n",
    "            # Try to find original file in input folder\n",
    "            original_name = None\n",
    "            \n",
    "            # Try PDF first\n",
    "            pdf_candidate = input_path / f\"{base_name}.pdf\"\n",
    "            if pdf_candidate.exists():\n",
    "                original_name = pdf_candidate.name\n",
    "            else:\n",
    "                # Try HTML\n",
    "                html_candidate = input_path / f\"{base_name}.html\"\n",
    "                if html_candidate.exists():\n",
    "                    original_name = html_candidate.name\n",
    "                else:\n",
    "                    # Try HTM\n",
    "                    htm_candidate = input_path / f\"{base_name}.htm\"\n",
    "                    if htm_candidate.exists():\n",
    "                        original_name = htm_candidate.name\n",
    "                    else:\n",
    "                        # If no original found, use the markdown filename as the name\n",
    "                        original_name = md_file_name\n",
    "            \n",
    "            print(f\"\\nðŸ”„ Processing existing markdown: {md_file_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Encode the markdown file content using encode_file_content\n",
    "                encoded_content = encode_file_content(str(md_file_path))\n",
    "                \n",
    "                if encoded_content:\n",
    "                    # Create JSON entry for this file\n",
    "                    file_info = {\n",
    "                        \"name\": original_name,\n",
    "                        \"content\": encoded_content,\n",
    "                        \"citable_url\": \"\"\n",
    "                    }\n",
    "                    files_info.append(file_info)\n",
    "                    print(f\"âœ“ Information added to JSON for: {original_name}\")\n",
    "                else:\n",
    "                    print(f\"âœ— Error encoding content of: {md_file_path}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"âœ— Error processing {md_file_name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    else:\n",
    "        # Normal conversion flow\n",
    "        # Delete and recreate output folder\n",
    "        if os.path.exists(output_folder):\n",
    "            shutil.rmtree(output_folder)\n",
    "            print(f\"ðŸ—‘ï¸  Deleted existing output folder: {output_folder}\")\n",
    "        \n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        print(f\"ðŸ“ Created output folder: {output_folder}\")\n",
    "        \n",
    "        # Get all PDF and HTML files from the input folder\n",
    "        input_path = pathlib.Path(input_folder)\n",
    "        pdf_files = list(input_path.glob(\"*.pdf\"))\n",
    "        html_files = list(input_path.glob(\"*.html\")) + list(input_path.glob(\"*.htm\"))\n",
    "        \n",
    "        all_files = pdf_files + html_files\n",
    "        \n",
    "        if not all_files:\n",
    "            print(f\"âš  No PDF or HTML files found in {input_folder}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"ðŸ“ Processing {len(all_files)} file(s) from {input_folder}...\")\n",
    "        \n",
    "        # Process each file\n",
    "        for file_path in all_files:\n",
    "            original_name = file_path.name\n",
    "            file_extension = file_path.suffix.lower()\n",
    "            \n",
    "            print(f\"\\nðŸ”„ Processing: {original_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Determine the output markdown file name\n",
    "                output_md_name = original_name.rsplit('.', 1)[0] + \".md\"\n",
    "                output_md_path = os.path.join(output_folder, output_md_name)\n",
    "                \n",
    "                # Convert based on file type\n",
    "                if file_extension == \".pdf\":\n",
    "                    if use_docling_for_pdf:\n",
    "                        # Use docling for PDF\n",
    "                        converter = DocumentConverter()\n",
    "                        result = converter.convert(str(file_path))\n",
    "                        markdown_content = result.document.export_to_markdown()\n",
    "                    else:\n",
    "                        # Use pymupdf4llm for PDF\n",
    "                        chunks = pymupdf4llm.to_markdown(str(file_path), page_chunks=False)\n",
    "                        if isinstance(chunks, list):\n",
    "                            markdown_content = \"\\n\\n\".join(chunks)\n",
    "                        else:\n",
    "                            markdown_content = chunks\n",
    "                elif file_extension in [\".html\", \".htm\"]:\n",
    "                    # Use docling for HTML\n",
    "                    converter = DocumentConverter()\n",
    "                    result = converter.convert(str(file_path))\n",
    "                    markdown_content = result.document.export_to_markdown()\n",
    "                else:\n",
    "                    print(f\"âš  Unsupported format: {file_extension}. Skipping...\")\n",
    "                    continue\n",
    "                \n",
    "                # Save the markdown file\n",
    "                with open(output_md_path, \"w\", encoding=\"utf-8\") as md_file:\n",
    "                    md_file.write(markdown_content)\n",
    "                print(f\"âœ“ Converted to: {output_md_path}\")\n",
    "                \n",
    "                # Encode the markdown file content using encode_file_content\n",
    "                encoded_content = encode_file_content(output_md_path)\n",
    "                \n",
    "                if encoded_content:\n",
    "                    # Create JSON entry for this file\n",
    "                    file_info = {\n",
    "                        \"name\": original_name,\n",
    "                        \"content\": encoded_content,\n",
    "                        \"citable_url\": \"\"\n",
    "                    }\n",
    "                    files_info.append(file_info)\n",
    "                    print(f\"âœ“ Information added to JSON for: {original_name}\")\n",
    "                else:\n",
    "                    print(f\"âœ— Error encoding content of: {output_md_path}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"âœ— Error processing {original_name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Save the JSON with all information\n",
    "    try:\n",
    "        with open(json_output_file, \"w\", encoding=\"utf-8\") as json_file:\n",
    "            json.dump(files_info, json_file, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\nâœ“ JSON saved to: {json_output_file}\")\n",
    "        print(f\"âœ“ Total files processed: {len(files_info)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error saving JSON: {e}\")\n",
    "    \n",
    "    return files_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files_from_json(json_file: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Loads the list of files from a JSON file created by process_folder.\n",
    "    \n",
    "    Args:\n",
    "        json_file: Path to the JSON file containing file information\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict]: List of files with \"name\", \"content\", and \"citable_url\" keys\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            files = json.load(f)\n",
    "        print(f\"âœ“ Loaded {len(files)} file(s) from {json_file}\")\n",
    "        return files\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âœ— JSON file not found: {json_file}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error loading JSON file: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Process a folder with PDF and HTML files\n",
    "input_folder = \"./markdown_output\"\n",
    "json_output_file = \"./files_info.json\"    # JSON file with the information\n",
    "\n",
    "# # Normal mode: converts PDF/HTML to Markdown (output_folder is created/deleted automatically)\n",
    "# '''\n",
    "# files = process_folder(\n",
    "#     input_folder=input_folder,\n",
    "#     json_output_file=json_output_file,\n",
    "#     skip_conversion=False\n",
    "# )\n",
    "# '''\n",
    "\n",
    "# Skip conversion mode: reads markdown files directly from input_folder\n",
    "process_folder(\n",
    "    input_folder=input_folder,\n",
    "    json_output_file=json_output_file,\n",
    "    skip_conversion=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = load_files_from_json(json_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Importing the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define set of custom entity types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_entity_types = [\n",
    "   \"PROCESSOR_COMPONENT\",\n",
    "    \"REGISTER\",\n",
    "    \"INSTRUCTION\",\n",
    "    \"HARDWARE_INTERFACE\",\n",
    "    \"MEMORY_UNIT\",\n",
    "    \"SIGNAL\",\n",
    "    \"EXCEPTION_TYPE\",\n",
    "    \"ARCHITECTURE_FEATURE\",\n",
    "    \"CONFIGURATION\",\n",
    "    \"MEASUREMENT\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit encoded file for Knowledge Graph Generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare import body\n",
    "file_content = encode_file_content(\"markdown_output/openrisc1200_spec_0.7_jp.md\")\n",
    "import_body = {\n",
    "    \"file_name\": \"openrisc1200_spec_0.7_jp.md\",\n",
    "    \"file_content\": file_content,\n",
    "    \"chunk_token_size\": 1200,  # Maximum tokens per text chunk (default: 1200)\n",
    "    \"enable_chunk_embeddings\": True,  # Generate embeddings for chunks (required for vector search)\n",
    "    \"entity_types\": custom_entity_types,\n",
    "    \"partition_id\": \"cadence_1\"\n",
    "}\n",
    "\n",
    "importer_response = send_request(\n",
    "    f\"/graphrag/importer/{importer_service_id}/v1/import\",\n",
    "    import_body,\n",
    "    \"POST\"\n",
    ")\n",
    "\n",
    "print(json.dumps(importer_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare import body\n",
    "file_content = encode_file_content(\"markdown_output/openrisc1200_spec.md\")\n",
    "import_body = {\n",
    "    \"file_name\": \"openrisc1200_spec.md\",\n",
    "    \"file_content\": file_content,\n",
    "    \"chunk_token_size\": 1200,  # Maximum tokens per text chunk (default: 1200)\n",
    "    \"enable_chunk_embeddings\": True,  # Generate embeddings for chunks (required for vector search)\n",
    "    \"entity_types\": custom_entity_types,\n",
    "    \"partition_id\": \"cadence_2\"\n",
    "}\n",
    "\n",
    "importer_response = send_request(\n",
    "    f\"/graphrag/importer/{importer_service_id}/v1/import\",\n",
    "    import_body,\n",
    "    \"POST\"\n",
    ")\n",
    "\n",
    "print(json.dumps(importer_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare import body\n",
    "file_content = encode_file_content(\"markdown_output/openrisc1200_supplementary_prm.md\")\n",
    "import_body = {\n",
    "    \"file_name\": \"openrisc1200_supplementary_prm.md\",\n",
    "    \"file_content\": file_content,\n",
    "    \"chunk_token_size\": 1200,  # Maximum tokens per text chunk (default: 1200)\n",
    "    \"enable_chunk_embeddings\": True,  # Generate embeddings for chunks (required for vector search)\n",
    "    \"entity_types\": custom_entity_types,\n",
    "    \"partition_id\": \"cadence_3\"\n",
    "}\n",
    "\n",
    "importer_response = send_request(\n",
    "    f\"/graphrag/importer/{importer_service_id}/v1/import\",\n",
    "    import_body,\n",
    "    \"POST\"\n",
    ")\n",
    "\n",
    "print(json.dumps(importer_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Creating a Retriever\n",
    "\n",
    "Start the GraphRAG Retriever service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the GraphRAG Retriever service\n",
    "retriever_params = {\n",
    "    \"db_name\": get_updated_env_var(\"DB_NAME\"),\n",
    "    \"username\": get_updated_env_var(\"USERNAME\"),\n",
    "    \"password\": get_updated_env_var(\"PASSWORD\"),\n",
    "    \"chat_model\": \"gpt-4.1\",\n",
    "    \"chat_api_provider\": \"openai\",\n",
    "    \"embedding_api_provider\": \"openai\",\n",
    "    \"chat_api_key\": get_updated_env_var(\"OPENAI_API_KEY\"),\n",
    "    \"embedding_api_key\": get_updated_env_var(\"OPENAI_API_KEY\"),\n",
    "    \"genai_project_name\": get_updated_env_var(\"DB_PROJECT_NAME\"),\n",
    "}\n",
    "\n",
    "retriever_response = start_service(\"arangodb-graphrag-retriever\", retriever_params)\n",
    "print(json.dumps(retriever_response, indent=2))\n",
    "\n",
    "# Extract service ID\n",
    "retriever_service_id = retriever_response[\"serviceInfo\"][\"serviceId\"].split(\"-\")[-1]\n",
    "print(f\"\\nRetriever Service ID: {retriever_service_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update retriever service params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare updated parameters\n",
    "updated_params = retriever_params.copy()\n",
    "updated_params[\"chat_model\"] = \"gpt-4o\"\n",
    "\n",
    "retriever_service_id_full = f\"arangodb-graphrag-retriever-{retriever_service_id}\"\n",
    "print(f\"Updating service: {retriever_service_id_full}\")\n",
    "response = update_service(retriever_service_id_full, updated_params)\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Getting a Streamed Response\n",
    "\n",
    "Query the knowledge graph with streaming response for real-time updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def send_streaming_query(url: str, payload: Dict) -> Tuple[List[Dict], str]:\n",
    "    \"\"\"Sends an async HTTP request expecting a streaming response (SSE-like).\"\"\"\n",
    "    if not jwt_token:\n",
    "        raise ValueError(\"JWT token is not set. Please authenticate first.\")\n",
    "    \n",
    "    authenticate()\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"text/event-stream\",\n",
    "        \"Authorization\": f\"Bearer {jwt_token}\"\n",
    "    }\n",
    "    \n",
    "    timeout = aiohttp.ClientTimeout(total=7200)\n",
    "    raw_response = \"\"\n",
    "    json_objects: List[Dict] = []\n",
    "    \n",
    "    try:\n",
    "        async with aiohttp.ClientSession(timeout=timeout, connector=aiohttp.TCPConnector(ssl=False)) as session:\n",
    "            async with session.post(url, json=payload, headers=headers) as response:\n",
    "                response.raise_for_status()\n",
    "                async for line in response.content:\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    line_str = line.decode(\"utf-8\", errors=\"replace\").strip()\n",
    "                    raw_response += line_str\n",
    "                    if not line_str:\n",
    "                        continue\n",
    "                    if line_str.startswith(\":\"):\n",
    "                        continue\n",
    "                    if line_str.startswith(\"data: \"):\n",
    "                        line_str = line_str[6:]\n",
    "                    try:\n",
    "                        obj = json.loads(line_str)\n",
    "                        json_objects.append(obj)\n",
    "                        print(f\"âœ“ Received: {obj}\")\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "                return json_objects, raw_response\n",
    "    except aiohttp.ClientError as e:\n",
    "        print(f\"ERROR: Error during streaming: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example streamed query\n",
    "query_body = {\n",
    "    \"query\": \"In Z-Tunnel 2.0, how does the MTU configuration differ between DTLS and TLS modes?\",\n",
    "    \"query_type\": 3,  # 1 for global, 2 for local/deep-search, 3 instant search\n",
    "}\n",
    "\n",
    "stream_url = f\"{get_updated_env_var('SERVER_URL')}/graphrag/retriever/{retriever_service_id}/v1/graphrag-query-stream\"\n",
    "\n",
    "print(f\"Streaming query to: {stream_url}\")\n",
    "print(f\"Query: {query_body['query']}\")\n",
    "\n",
    "try:\n",
    "    asyncio.get_running_loop()\n",
    "    events, raw = await send_streaming_query(stream_url, query_body)\n",
    "except RuntimeError:\n",
    "    events, raw = asyncio.run(send_streaming_query(stream_url, query_body))\n",
    "\n",
    "print(f\"âœ“ Events received: {len(events)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Getting a Full Response\n",
    "\n",
    "Query the knowledge graph and get the complete response in one request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full response query\n",
    "query_body = {\n",
    "    \"query\": \"In Z-Tunnel 2.0, how does the MTU configuration differ between DTLS and TLS modes?\",\n",
    "    \"query_type\": 2,  # 1 for global, 2 for local/deep-search, 3 instant search\n",
    "}\n",
    "\n",
    "full_response = send_request(\n",
    "    f\"/graphrag/retriever/{retriever_service_id}/v1/graphrag-query\",\n",
    "    query_body,\n",
    "    \"POST\"\n",
    ")\n",
    "\n",
    "print(\"Full Response:\")\n",
    "print(json.dumps(full_response, indent=2))\n",
    "print(\"\\nResult:\")\n",
    "print(full_response.get(\"result\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup: Stop Services\n",
    "\n",
    "Remember to stop services when finished to free up resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_service(service_id: str) -> Optional[Dict]:\n",
    "    \"\"\"Stop a service using the gen-ai API.\"\"\"\n",
    "    body = {\"service_id\": service_id}\n",
    "    return send_request(f\"/gen-ai/v1/service/{service_id}\", body, \"DELETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Importer service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importer_stop_response = stop_service(f\"arangodb-graphrag-importer-{importer_service_id}\")\n",
    "print(json.dumps(importer_stop_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Retriever service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_stop_response = stop_service(f\"arangodb-graphrag-retriever-{retriever_service_id}\")\n",
    "print(json.dumps(retriever_stop_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook provides a complete, production-ready workflow for using Arango GraphRAG services:\n",
    "\n",
    "1. **GenAI API** â€“ Authentication and request handling\n",
    "2. **Starting Services** â€“ Launch Importer and Retriever\n",
    "3. **Service Management** â€“ List, upgrade, and stop services\n",
    "4. **Document Processing** â€“ Convert PDFs to Markdown\n",
    "5. **Knowledge Graph Import** â€“ Upload documents for processing\n",
    "6. **Querying** â€“ Retrieve streamed, full, or context-only responses\n",
    "7. **Kubernetes Integration** â€“ Use k9s for pod monitoring\n",
    "\n",
    "All code is fully functional and well-documented for easy integration into your projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
